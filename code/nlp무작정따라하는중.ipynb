{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82bEIOEh1rbk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, re\n",
        "import argparse\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import datetime\n",
        "import time\n",
        "from itertools import combinations\n",
        "\n",
        "from transformers import *\n",
        "import torch\n",
        "import torch.nn.init\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CZ31BFij1rbn"
      },
      "outputs": [],
      "source": [
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfcD4RPc1rbn"
      },
      "outputs": [],
      "source": [
        "def train_model(args):\n",
        "  '''\n",
        "  [1. 전처리 과정에서 생성된 데이터 호출]\n",
        "  [2. 텐서 생성]\n",
        "  '''\n",
        "  set_seed(args)\n",
        "\n",
        "  train_data = pd.read_csv('train.csv')\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(train_data['cat3'].unique())\n",
        "\n",
        "  N = train_data.shape[0]\n",
        "  MAX_LEN = 512\n",
        "  \n",
        "  train_idx, valid_idx = train_test_split(np.arange(N))\n",
        "\n",
        "  # training\n",
        "  N = train_idx.shape[0]\n",
        "  overview = train_data.loc[train_idx, 'overview'].values\n",
        "  cat3 = encoder.transform(train_data.loc[train_idx, 'cat3'].values)\n",
        "\n",
        "  input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
        "  attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
        "  labels = np.zeros((N), dtype=int)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
        "\n",
        "  for i in tqdm(range(N), position=0, leave=True):\n",
        "    try:\n",
        "      cur_ov = str(overview[i])\n",
        "      encoded_input = tokenizer(cur_ov, return_tensors='pt', max_length=512,\n",
        "                                padding='max_length', truncation=True)\n",
        "      input_ids[i, ] = encoded_input['input_ids']\n",
        "      attention_masks[i, ] = encoded_input['attention_mask']\n",
        "      labels[i] = cat3[i]\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass\n",
        "  \n",
        "  # validating\n",
        "  N = valid_idx.shape[0]\n",
        "\n",
        "  overview = train_data.loc[valid_idx, 'overview'].values\n",
        "  cat3 = encoder.transform(train_data.loc[valid_idx, 'overview'].values)\n",
        "\n",
        "  valid_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
        "  valid_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
        "  valid_labels = np.zeros((N), dtype=int)\n",
        "\n",
        "  for i in tqdm(range(N), position=0, leave=True):\n",
        "    try:\n",
        "        cur_ov = str(overview[i])\n",
        "        encoded_input = tokenizer(cur_ov, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "        valid_input_ids[i, ] = encoded_input['input_ids']\n",
        "        valid_attention_masks[i, ] = encoded_input['attention_mask']\n",
        "        valid_labels[i] = cat3[i]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "    if os.path.exists(args.dir_path):\n",
        "        os.makedirs(args.dir_path, exist_ok=True)\n",
        "    \n",
        "    print(\"\\n\\nMake tensor\\n\\n\")\n",
        "    input_ids = torch.tensor(input_ids, dtype=int)\n",
        "    attention_masks = torch.tensor(attention_masks, dtype=int)\n",
        "    labels= torch.tensor(labels, dtype=int)\n",
        "\n",
        "    valid_input_ids = torch.tensor(valid_input_ids, dtype=int)\n",
        "    valid_attention_masks = torch.tensor(valid_attention_masks, dtype=int)\n",
        "    valid_labels = torch.tensor(valid_labels, dtype=int)\n",
        "\n",
        "\n",
        "    if args.save_tensor == True:\n",
        "        torch.save(input_ids, \"./data/\"+args.dir_path+\"/train_input_ids_1012.pt\")\n",
        "        torch.save(attention_masks, \"./data/\"+args.dir_path+\"train_attention_masks_1012.pt\")\n",
        "        torch.save(labels, \"./data/\"+args.dir_path+\"train_labels_1012.pt\")\n",
        "\n",
        "        torch.save(valid_input_ids, \"./data/\"+args.dir_path+\"/valid_input_ids_1012.pt\")\n",
        "        torch.save(attention_masks, \"./data/\"+args.dir_path+\"/valid_attention_masks_1012.pt\")\n",
        "        torch.save(valid_labels, \"./data/\"+args.dir_path+\"valid_labels_1012.pt\")\n",
        "    \n",
        "\n",
        "\n",
        "    # Setup training\n",
        "    def flat_accuracy(preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat==labels_flat)/len(labels/flat)\n",
        "    \n",
        "    def format_time(elapsed):\n",
        "        elapsed_rounded = int(round((elapsed)))\n",
        "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "    train_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n",
        "\n",
        "    validation_data = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=args.batch_size)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint_path)\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "    total_steps = len(train_dataloader) * args.epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    # Train\n",
        "    train_losses, train_accuracies = [], []\n",
        "    val_losses, val_accuracies = [], []\n",
        "    model.zero_grad()\n",
        "    for i in range(args.epochs):\n",
        "        print(\"\")\n",
        "        print('======= Epoch {:} / {:} ======'.format(i+1, args.epochs))\n",
        "        print('Training...')\n",
        "        t0 = time.time()\n",
        "        train_loss, train_accuracy = 0, 0\n",
        "        model.train()\n",
        "        for step, batch in tqdm(enumerate(train_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
        "            if step%10000 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() -t0)\n",
        "                print('\\tBatch {:>5} of {:>5}. \\tElapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "                print('\\tcurrent average loss = {}'.format(train_loss/step))\n",
        "\n",
        "                batch = tuple(t.to(device) for t t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "                outputs = model(b_input_ids, attention_masks=b_input_mask, labels=b_labels)\n",
        "                loss = outputs[0]\n",
        "                logits = outputs[1]\n",
        "                train_loss += loss.item()\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.detach().cpu().numpy()\n",
        "                train_accuracy += flat_accuracy(logits, label_ids)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "        \n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        avg_train_accuracy = train_accuracy / len(train_dataloader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(avg_train_accuracy)\n",
        "        print(\"\\tAverage training loss: {0:.8f}\",format(avg_train_loss))\n",
        "        print(\"\\tAverage training accuracy: {0:.8f}\".format(avg_train_accuracy))\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Validating...\")\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "        val_loss, val_accuracy = 0, 0\n",
        "        for step, batch in tqdm(enumerate(validation_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            with torch.no_grad():\n",
        "                outputs = model(b_input_ids, attention_masks=b_input_mask)\n",
        "            \n",
        "            logits = outputs[0]\n",
        "            logits = logits.detach().cpu()\n",
        "            labels_ids = b_labels.detach().cpu()\n",
        "\n",
        "            logits = logits.numpy()\n",
        "            label_ids = label_ids.numpy()\n",
        "            val_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        avg_val_accuracy = val_accuracy/len(validation_dataloader)\n",
        "        avg_val_loss = val_loss/len(validation_dataloader)\n",
        "        val_accuracies.append(avg_val_accuracy)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(\"\\tAverage validation loss: {0:.8f}\".format(avg_val_loss))\n",
        "        print(\"\\tAverage validation accuracy: {0:.8f}\".format(avg_val_accuracy))\n",
        "        print(\"\\tTraining epoch took: {:}\".format(format_time(time.time()-t0)))\n",
        "\n",
        "        # if np.min(val_losses) == val_losses[-1]:\n",
        "        print(\"saving current best checkpoint\")\n",
        "        torch.save(model.state_dict(), \"./data/{}/{}th_1012.pt\".format(args.dir_path, i+1))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference_model(args):\n",
        "    test_data = pd.read_csv(PATH_MOLU)\n",
        "\n",
        "    overview = test_data['overview'].values\n",
        "\n",
        "    N = test_data.shape[0]\n",
        "    MAX_LEN = 512\n",
        "\n",
        "    test_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
        "    test_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
        "\n",
        "    tokenizer = AutoTokenizer.form_pretrained(args.checkpoint_path)\n",
        "    tokenizer.truncation_side = \"right\"\n",
        "\n",
        "    for i in tqdm(range(N), position=0, leave=True):\n",
        "        try:\n",
        "            cur_ov = str(overview[i])\n",
        "            encoded_input = tokenizer(cur_ov, return_tensors='pt', max_length=MAX_LEN, padding='max_length', truncation=True)\n",
        "            test_input_ids[i, ] = encoded_input['input_ids']\n",
        "            test_attention_masks[i, ] = encoded_input['attention_mask']\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            pass\n",
        "    \n",
        "    test_input_ids = torch.tensor(test_input_ids, dtype=int)\n",
        "    test_attention_masks = torch.tensor(test_attention_masks, dtype=int)\n",
        "\n",
        "    if args.save_tensor == True:\n",
        "        torch.save(test_input_ids, \"./data/{}/test_input_ids_1012.pt\".format(args.dir_path))\n",
        "        torch.save(test_attention_masks, \"./data/{}/test_attention_masks_1012.pt\".format(args.dir_path))\n",
        "    \n",
        "    model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint_path)\n",
        "    PATH = \"몰루?\"\n",
        "\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    model.cuda()\n",
        "\n",
        "    test_tensor = TensorDataset(test_input_ids, test_attention_masks)\n",
        "    test_sampler = SequentialSampler(test_tensor)\n",
        "    test_dataloader = DataLoader(test_tensor, sampler=test_sampler, batch_size=args.test_batch_size)\n",
        "\n",
        "    submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    preds = np.array([])\n",
        "    for step"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
